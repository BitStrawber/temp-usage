# utils.py

# ----------------- å¯¼å…¥æ‰€éœ€åº“ -----------------
import torch
import esm
import numpy as np
from tqdm import tqdm
import os
import warnings

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# ----------------- å‡½æ•°å®šä¹‰ -----------------

def check_device():
    """æ£€æŸ¥å¯ç”¨çš„è®¡ç®—è®¾å¤‡ (GPUæˆ–CPU) å¹¶è¿”å›ã€‚"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device}")
    return device


def load_esm2_model(model_name="esm2_t12_35M_UR50D", device='cpu'):
    """
    åŠ è½½æŒ‡å®šåç§°çš„ESM-2æ¨¡å‹ã€è¯å…¸å’Œæ‰¹æ¬¡è½¬æ¢å™¨ã€‚
    åˆ©ç”¨ torch.hub çš„æœ¬åœ°ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤ä¸‹è½½ã€‚
    """
    print(f"æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...")
    try:
        model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)
        batch_converter = alphabet.get_batch_converter()
        model.eval()
        model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
        return model, alphabet, batch_converter
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½ESMæ¨¡å‹ '{model_name}' å¤±è´¥ã€‚åŸå› : {e}")
        print("\nğŸš¨ è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æœ€æ–°ç‰ˆ ESM åº“: `pip install fair-esm`")
        print(f"   ESM ä¼šè‡ªåŠ¨å°è¯•ä¸‹è½½æ¨¡å‹ã€‚å¦‚æœå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨å°†æ¨¡å‹æ–‡ä»¶ '{model_name}.pt' æ”¾ç½®äºç¼“å­˜ç›®å½•ã€‚")
        exit(1)


def get_protein_embeddings_batch(sequences, model, alphabet, batch_converter, device, layer=12, batch_size=32):
    """
    ä½¿ç”¨ESM-2æ¨¡å‹ï¼Œåˆ†æ‰¹æ¬¡åœ°ä¸ºè›‹ç™½è´¨åºåˆ—ç”ŸæˆåµŒå…¥å‘é‡(embeddings)ã€‚
    """
    if not sequences or all(s is None or s == '' for s in sequences):
        print("â„¹ï¸ æç¤º: åºåˆ—åˆ—è¡¨ä¸ºç©ºï¼Œå°†è¿”å›ç©ºåµŒå…¥å‘é‡æ•°ç»„ã€‚")
        return np.array([])

    print(f"æ­£åœ¨ä¸º {len(sequences)} æ¡åºåˆ—ç”ŸæˆåµŒå…¥å‘é‡...")
    all_embeddings = []

    for i in tqdm(range(0, len(sequences), batch_size), desc="æ­£åœ¨åˆ†æ‰¹å¤„ç†åºåˆ—"):
        batch_sequences = sequences[i:i + batch_size]
        data = [(f"protein_{j}", seq) for j, seq in enumerate(batch_sequences)]

        batch_labels, batch_strs, batch_tokens = batch_converter(data)
        batch_tokens = batch_tokens.to(device)
        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)

        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[layer], return_contacts=False)

        token_representations = results["representations"][layer]
        batch_embeddings = []
        for j, tokens_len in enumerate(batch_lens):
            # å¯¹æ¯ä¸ªåºåˆ—çš„è¡¨å¾è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œå¿½ç•¥èµ·å§‹(BOS)å’Œç»“æŸ(EOS)æ ‡è®°
            seq_repr = token_representations[j, 1:tokens_len - 1].mean(0)
            batch_embeddings.append(seq_repr.cpu().numpy())

        all_embeddings.extend(batch_embeddings)

    return np.array(all_embeddings)