import pandas as pd
import numpy as np
import os
import argparse
import joblib
from tqdm import tqdm
import torch
from sklearn.linear_model import RidgeCV
from transformers import AutoTokenizer, AutoModelForMaskedLM
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ==============================================================================
# è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
# ==============================================================================
# ... (check_device, load_esm2_model, get_protein_embeddings_batch å‡½æ•°æ— éœ€ä¿®æ”¹)
def check_device():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    return device

def load_esm2_model(model_name, device):
    print(f"æ­£åœ¨ä» '{model_name}' åŠ  à¤²à¥‹à¤¡ ESM-2 æ¨¡å‹...")
    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print("âœ… ESM-2 æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    return model, tokenizer

def get_protein_embeddings_batch(sequences, model, tokenizer, device, repr_layer, batch_size):
    embeddings = []
    model.eval()
    for i in tqdm(range(0, len(sequences), batch_size), desc="ç”ŸæˆåµŒå…¥å‘é‡"):
        batch_seqs = sequences[i:i + batch_size]
        inputs = tokenizer(batch_seqs, return_tensors="pt", padding=True, truncation=True, max_length=1024).to(device)
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states[repr_layer]
        for j in range(len(batch_seqs)):
            seq_len = inputs['attention_mask'][j].sum().item()
            seq_embedding = hidden_states[j, 1:seq_len - 1].mean(dim=0).cpu().numpy()
            embeddings.append(seq_embedding)
    return np.array(embeddings)
# ==============================================================================
# --- ä¿®æ”¹: æ¨¡å‹æ€§èƒ½å¯è§†åŒ–å‡½æ•° ---
# ==============================================================================

def plot_training_performance(y_train, y_pred_train, y_test, y_pred_test, output_path):
    """
    (æ‰©å±•ç‰ˆ) ç»˜åˆ¶ 2x2 é¢æ¿ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬æ®‹å·®å›¾ã€‚
    """
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)

    print(f"\n--- æ¨¡å‹æ€§èƒ½è¯„ä¼° ---")
    print(f"  è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
    print(f"  æµ‹è¯•é›† RÂ²: {test_r2:.4f}")

    plt.style.use('seaborn-v0_8-whitegrid')
    # --- ä¿®æ”¹: åˆ›å»ºä¸€ä¸ª 2x2 çš„å­å›¾å¸ƒå±€ ---
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))
    fig.suptitle('Scoring Model Performance Evaluation', fontsize=18)

    # --- å›¾ 1: è®­ç»ƒé›†é¢„æµ‹ vs. çœŸå® ---
    sns.scatterplot(x=y_train, y=y_pred_train, alpha=0.6, color='blue', ax=axes[0, 0], s=20)
    min_val_train = min(y_train.min(), y_pred_train.min())
    max_val_train = max(y_train.max(), y_pred_train.max())
    axes[0, 0].plot([min_val_train, max_val_train], [min_val_train, max_val_train], 'r--', lw=2)
    axes[0, 0].set_xlabel('True Score (Training Set)', fontsize=12)
    axes[0, 0].set_ylabel('Predicted Score', fontsize=12)
    axes[0, 0].set_title(f'Performance on Training Set (RÂ² = {train_r2:.4f})', fontsize=14)
    axes[0, 0].grid(True)

    # --- å›¾ 2: æµ‹è¯•é›†é¢„æµ‹ vs. çœŸå® ---
    sns.scatterplot(x=y_test, y=y_pred_test, alpha=0.6, color='green', ax=axes[0, 1], s=20)
    min_val_test = min(y_test.min(), y_pred_test.min())
    max_val_test = max(y_test.max(), y_pred_test.max())
    axes[0, 1].plot([min_val_test, max_val_test], [min_val_test, max_val_test], 'r--', lw=2)
    axes[0, 1].set_xlabel('True Score (Test Set)', fontsize=12)
    axes[0, 1].set_ylabel('Predicted Score', fontsize=12)
    axes[0, 1].set_title(f'Performance on Test Set (RÂ² = {test_r2:.4f})', fontsize=14)
    axes[0, 1].grid(True)

    # --- å›¾ 3 (æ–°å¢): è®­ç»ƒé›†æ®‹å·®å›¾ ---
    train_residuals = y_train - y_pred_train
    sns.scatterplot(x=y_pred_train, y=train_residuals, alpha=0.6, color='blue', ax=axes[1, 0], s=20)
    axes[1, 0].axhline(y=0, color='r', linestyle='--')
    axes[1, 0].set_xlabel('Predicted Score (Training Set)', fontsize=12)
    axes[1, 0].set_ylabel('Residuals (True - Predicted)', fontsize=12)
    axes[1, 0].set_title('Residuals on Training Set', fontsize=14)
    axes[1, 0].grid(True)

    # --- å›¾ 4 (æ–°å¢): æµ‹è¯•é›†æ®‹å·®å›¾ ---
    test_residuals = y_test - y_pred_test
    sns.scatterplot(x=y_pred_test, y=test_residuals, alpha=0.6, color='green', ax=axes[1, 1], s=20)
    axes[1, 1].axhline(y=0, color='r', linestyle='--')
    axes[1, 1].set_xlabel('Predicted Score (Test Set)', fontsize=12)
    axes[1, 1].set_ylabel('Residuals (True - Predicted)', fontsize=12)
    axes[1, 1].set_title('Residuals on Test Set', fontsize=14)
    axes[1, 1].grid(True)

    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    plt.savefig(output_path, dpi=300)
    print(f"\nâœ… æ¨¡å‹æ€§èƒ½å›¾è¡¨ (4-panel) å·²ä¿å­˜è‡³: {output_path}")
    plt.close(fig)

# ==============================================================================
# --- ä¿®æ”¹: é›†æˆæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°é€»è¾‘ ---
# ==============================================================================
def train_model(data_path: str, model_output_path: str, base_model: str, repr_layer: int, batch_size: int):
    """
    (ä¿®æ”¹ç‰ˆ) ä½¿ç”¨ESM-2ç”ŸæˆåµŒå…¥å‘é‡ï¼Œåˆ’åˆ†æ•°æ®ï¼Œè®­ç»ƒå›å½’æ¨¡å‹ï¼Œ
    è¯„ä¼°å…¶åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚
    """
    print("=" * 80)
    print(f"å¼€å§‹è®­ç»ƒå’Œè¯„ä¼°æ‰“åˆ†æ¨¡å‹...")
    print(f" -> è®­ç»ƒæ•°æ®: {data_path}")
    print(f" -> ESM-2æ¨¡å‹: {base_model} (ä½¿ç”¨ç¬¬ {repr_layer} å±‚)")

    device = check_device()
    esm_model, tokenizer = load_esm2_model(base_model, device)
    df_train_full = pd.read_csv(data_path)

    sequences_full = df_train_full['sequence'].tolist()
    labels_full = df_train_full['label'].values
    print(f"å…± {len(sequences_full)} æ¡åºåˆ—ç”¨äºè®­ç»ƒå’Œæµ‹è¯•ã€‚")
    embeddings_full = get_protein_embeddings_batch(
        sequences_full, esm_model, tokenizer, device, repr_layer, batch_size
    )

    X_train, X_test, y_train, y_test = train_test_split(
        embeddings_full, labels_full, test_size=0.2, random_state=42
    )
    print(f"\næ•°æ®å·²åˆ’åˆ†ä¸º: {len(X_train)} (è®­ç»ƒ) / {len(X_test)} (æµ‹è¯•) æ ·æœ¬ã€‚")

    print("å¼€å§‹åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒå›å½’æ¨¡å‹...")
    scoring_model = RidgeCV(alphas=np.logspace(-3, 3, 10), cv=5)
    scoring_model.fit(X_train, y_train)
    print(f"å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆã€‚äº¤å‰éªŒè¯é€‰å‡ºçš„æœ€ä½³alphaå€¼ä¸º: {scoring_model.alpha_:.4f}")

    # --- åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ä»¥ä¾›è¯„ä¼° ---
    y_pred_train = scoring_model.predict(X_train)
    y_pred_test = scoring_model.predict(X_test)

    # --- è°ƒç”¨åŒå›¾å¯è§†åŒ–å‡½æ•° ---
    performance_plot_path = model_output_path.replace(".joblib", "_performance.png")
    plot_training_performance(y_train, y_pred_train, y_test, y_pred_test, performance_plot_path)

    joblib.dump(scoring_model, model_output_path)
    print(f"\nâœ… æ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³: {model_output_path}")
    print("=" * 80)

# ... (score_data, extract_dms_id_from_path, find_and_combine_data, å’Œ main å‡½æ•°ä¿æŒä¸å˜)
# ... (è¿™äº›å‡½æ•°çš„å†…å®¹ä¸æ‚¨ä¸Šä¸€ç‰ˆä»£ç å®Œå…¨ç›¸åŒ)
def score_data(model_path: str, base_model: str, repr_layer: int, batch_size: int, data_to_score_path: str,
               output_file_path: str):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„å›å½’æ¨¡å‹ï¼Œä¸ºæ–°çš„è›‹ç™½è´¨åºåˆ—è¿›è¡Œæ‰“åˆ†ã€‚
    """
    print("=" * 80)
    print(f"å¼€å§‹ä½¿ç”¨å›å½’æ¨¡å‹ä¸ºå¤šä¸ªåºåˆ—åˆ—è¿›è¡Œæ‰“åˆ†...")
    print(f" -> æ‰“åˆ†æ¨¡å‹: {model_path}")
    print(f" -> ESM-2æ¨¡å‹: {base_model} (ä½¿ç”¨ç¬¬ {repr_layer} å±‚)")
    print(f" -> å¾…æ‰“åˆ†æ•°æ®: {data_to_score_path}")

    # 1. åˆå§‹åŒ–å’ŒåŠ è½½
    device = check_device()
    df = pd.read_csv(data_to_score_path)
    scoring_model = joblib.load(model_path)
    # --- ä¿®æ”¹ï¼šæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼ (æ­¤è¡Œå·²æ­£ç¡®ï¼Œä½†ä¸ºä¿æŒä¸€è‡´æ€§è€Œå±•ç¤º) ---
    esm_model, tokenizer = load_esm2_model(base_model, device)

    # 2. å®šä¹‰éœ€è¦è¢«æ‰“åˆ†çš„åºåˆ—åˆ—
    sequence_columns_to_score = [
        'mutated_sequence',
        'predicted_sequence_trained',
        'predicted_sequence'
    ]

    for col in sequence_columns_to_score:
        print(f"\n--- æ­£åœ¨å¤„ç†åˆ—: '{col}' ---")

        if col not in df.columns:
            print(f"âš ï¸ è­¦å‘Š: æŒ‡å®šçš„åˆ— '{col}' åœ¨CSVæ–‡ä»¶ä¸­ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
            continue

        sequences_to_score = df[col].dropna().tolist()
        if not sequences_to_score:
            print(f"â„¹ï¸ åˆ— '{col}' ä¸­æ²¡æœ‰æœ‰æ•ˆåºåˆ—ï¼Œè·³è¿‡ã€‚")
            continue

        # --- ä¿®æ”¹ï¼šç§»é™¤ batch_converter å‚æ•° ---
        embeddings = get_protein_embeddings_batch(
            sequences_to_score, esm_model, tokenizer, device, repr_layer, batch_size
        )

        print("åµŒå…¥å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œå¼€å§‹é¢„æµ‹åˆ†æ•°...")
        predictions = scoring_model.predict(embeddings)

        pred_series = pd.Series(predictions, index=df[col].dropna().index)

        output_col_name = f"predicted_score_{col}"
        df[output_col_name] = pred_series
        df[output_col_name] = df[output_col_name].round(9)
        print(f"âœ… å·²ç”Ÿæˆé¢„æµ‹åˆ†æ•°å¹¶æ·»åŠ åˆ°æ–°åˆ— '{output_col_name}'ã€‚")

    # ... åç»­åˆ†æå’Œä¿å­˜éƒ¨åˆ†æ— éœ€ä¿®æ”¹ ...
    print("\n--- æ­£åœ¨è¿›è¡Œä¼˜åŒ–åˆ†æå’Œæ ‡æ³¨ ---")
    score_trained = 'predicted_score_predicted_sequence_trained'
    score_gen = 'predicted_score_predicted_sequence'
    original_score_col = 'DMS_score'
    required_score_cols = [score_trained, score_gen, original_score_col]

    improvement_count = 0
    if all(col in df.columns for col in required_score_cols):
        condition = (df[score_trained] > df[score_gen]) & \
                    (df[score_gen] > df[original_score_col])
        df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = np.where(condition, 'æ˜¯', 'å¦')
        improvement_count = df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'].value_counts().get('æ˜¯', 0)
        print(f"åˆ†æå®Œæˆã€‚æ–°åˆ— 'æ˜¯å¦ä¼˜åŒ–æˆåŠŸ' å·²æ·»åŠ ã€‚")
        print(f"ğŸ“ˆ ä¼˜åŒ–æˆåŠŸ (å¾®è°ƒåˆ† > åŸºç¡€åˆ† > åŸå§‹DMSåˆ†) çš„åºåˆ—æ•°é‡: {improvement_count}")
    else:
        missing_cols = [col for col in required_score_cols if col not in df.columns]
        print(f"âš ï¸ è­¦å‘Š: ç¼ºå°‘è¿›è¡Œåˆ†ææ‰€éœ€çš„åˆ—: {missing_cols}ã€‚å·²è·³è¿‡ä¼˜åŒ–åˆ†æã€‚")
        df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = 'åˆ†æè·³è¿‡'

    summary_data = {col: '' for col in df.columns}
    summary_data[df.columns[0]] = "--- ç»Ÿè®¡ç»“æœ ---"
    if 'æ˜¯å¦ä¼˜åŒ–æˆåŠŸ' in df.columns and improvement_count > 0:
        summary_data['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = f"æ€»è®¡ä¼˜åŒ–æˆåŠŸ: {improvement_count}"

    summary_df = pd.DataFrame([summary_data])
    results_df = pd.concat([df, summary_df], ignore_index=True)

    results_df.to_csv(output_file_path, index=False, float_format='%.9f')
    print(f"\nâœ… æ‰“åˆ†å’Œåˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
    print("=" * 80)

def extract_dms_id_from_path(path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–DMS_idã€‚"""
    try:
        return os.path.basename(os.path.dirname(path))
    except Exception:
        raise ValueError(f"æ— æ³•ä»è·¯å¾„ {path} ä¸­æå–DMS_idã€‚")

def find_and_combine_data(target_dms_id: str, dms_data_dir: str, metadata_file_path: str = None) -> pd.DataFrame:
    """
    (å‡€åŒ–ç‰ˆ) æ ¹æ®æ˜¯å¦æä¾›å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé€‰æ‹©æ•°æ®æ‰©å±•ç­–ç•¥ã€‚
    ç§»é™¤äº†å¤§éƒ¨åˆ†è°ƒè¯•ä¿¡æ¯ï¼Œä¿ç•™æ ¸å¿ƒæµç¨‹æ—¥å¿—ã€‚
    """
    combined_df_list = []

    # --- æ¨¡å¼ 1: æä¾›äº†å…ƒæ•°æ®æ–‡ä»¶ï¼Œè¿›è¡Œå¤§è§„æ¨¡æ‰©å±• ---
    if metadata_file_path:
        print("\n--- æ¨¡å¼: ä½¿ç”¨å…ƒæ•°æ®è¿›è¡Œæ•°æ®é›†æ‰©å±• ---")
        if not os.path.exists(metadata_file_path):
             raise FileNotFoundError(f"æŒ‡å®šçš„å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file_path}")

        try:
            metadata_df = pd.read_csv(metadata_file_path)

            target_protein_metadata = metadata_df[metadata_df['DMS_id'] == target_dms_id]
            if target_protein_metadata.empty:
                raise ValueError(f"DMS_id '{target_dms_id}' åœ¨å…ƒæ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")

            target_assay = target_protein_metadata['selection_assay'].iloc[0]
            target_title = target_protein_metadata['title'].iloc[0]
            print(f"\nç›®æ ‡è›‹ç™½è´¨ '{target_dms_id}' çš„å±æ€§:")
            print(f" -> selection_assay: {target_assay}")
            print(f" -> title: {target_title}")

            matching_proteins = metadata_df[
                (metadata_df['selection_assay'] == target_assay) &
                (metadata_df['title'] == target_title)
            ]
            print(f"\nåœ¨å…ƒæ•°æ®ä¸­æ‰¾åˆ° {len(matching_proteins)} ä¸ªåŒ¹é…çš„æ•°æ®é›†:")

            for _, row in matching_proteins.iterrows():
                dms_id = row['DMS_id']
                dms_filename = row['DMS_filename']
                file_path = os.path.join(dms_data_dir, dms_id, dms_filename)
                print(f" -> æ­£åœ¨æŸ¥æ‰¾æ•°æ®: {file_path}")

                if os.path.exists(file_path):
                    df = pd.read_csv(file_path)
                    if 'mutated_sequence' in df.columns and 'DMS_score' in df.columns:
                        combined_df_list.append(df[['mutated_sequence', 'DMS_score']])
                    else:
                        print(f"    [è­¦å‘Š] æ–‡ä»¶ {dms_filename} ç¼ºå°‘å¿…éœ€åˆ—ã€‚")
                else:
                    print(f"    [è­¦å‘Š] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
        except Exception as e:
            # ä¿ç•™é¡¶å±‚çš„é”™è¯¯æ•è·ï¼Œä»¥ä¾¿åœ¨ç­›é€‰æˆ–è¯»å–è¿‡ç¨‹ä¸­å‡ºé”™æ—¶ä»èƒ½æä¾›ä¿¡æ¯
            print(f"âŒ åœ¨å¤„ç†å…ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise e

    # --- æ¨¡å¼ 2: æœªæä¾›å…ƒæ•°æ®ï¼Œåªä½¿ç”¨è‡ªèº«æ•°æ® ---
    else:
        print(f"\n--- æ¨¡å¼: ä»…ä½¿ç”¨ç›®æ ‡ '{target_dms_id}' è‡ªèº«çš„æ•°æ®è¿›è¡Œè®­ç»ƒ ---")
        self_data_filename = f"{target_dms_id}.csv"
        file_path = os.path.join(dms_data_dir, target_dms_id, self_data_filename)
        print(f" -> æ­£åœ¨æŸ¥æ‰¾è‡ªèº«æ•°æ®: {file_path}")

        file_found = False
        if os.path.exists(file_path):
            file_found = True
        else:
            alt_filename = "DMS_substitutions.csv"
            file_path = os.path.join(dms_data_dir, target_dms_id, alt_filename)
            print(f" -> è‡ªèº«æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå°è¯•å¤‡ç”¨åç§°: {file_path}")
            if os.path.exists(file_path):
                file_found = True

        if file_found:
            try:
                df = pd.read_csv(file_path)
                if 'mutated_sequence' in df.columns and 'DMS_score' in df.columns:
                    combined_df_list.append(df[['mutated_sequence', 'DMS_score']])
                else:
                    print(f"    [è­¦å‘Š] æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—ã€‚")
            except Exception as e:
                print(f"    [é”™è¯¯] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}ã€‚")
        else:
            print(f"    [è­¦å‘Š] è‡ªèº«æ•°æ®æ–‡ä»¶å’Œå¤‡ç”¨æ–‡ä»¶å‡ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")


    if not combined_df_list:
        raise RuntimeError("æœªèƒ½åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®æ¥æ„å»ºè®­ç»ƒé›†ã€‚")

    combined_df = pd.concat(combined_df_list, ignore_index=True)
    print(f"\næ•°æ®é›†åˆå¹¶å®Œæˆï¼Œå…± {len(combined_df)} æ¡è®°å½•ç”¨äºè®­ç»ƒã€‚")
    return combined_df

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨çµæ´»çš„æ•°æ®æºè‡ªåŠ¨åŒ–è›‹ç™½è´¨æ‰“åˆ†æµç¨‹ã€‚")
    parser.add_argument("--target_data_path", type=str, required=True, help="å¾…æ‰“åˆ†çš„ç›®æ ‡æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--metadata_file", type=str, default=None, help="(å¯é€‰) å…ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œå°†è¿›è¡Œæ•°æ®é›†æ‰©å±•ã€‚")
    parser.add_argument("--dms_data_dir", type=str, required=True, help="åŒ…å«æ‰€æœ‰DMSæ•°æ®å­ç›®å½•çš„æ ¹ç›®å½•ã€‚")
    parser.add_argument("--output_base_dir", type=str, required=True, help="ä¿å­˜æœ€ç»ˆç»“æœçš„åŸºç¡€ç›®å½•ã€‚")
    parser.add_argument("--base_model", type=str, default="../esm2_model_local", help="ESM-2æ¨¡å‹è·¯å¾„ã€‚")
    parser.add_argument("--repr_layer", type=int, default=12, help="ESM-2æå–è¡¨å¾çš„å±‚ã€‚")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹å¤„ç†å¤§å°ã€‚")
    args = parser.parse_args()

    target_dms_id = extract_dms_id_from_path(args.target_data_path)
    final_output_dir = os.path.join(args.output_base_dir, target_dms_id)
    os.makedirs(final_output_dir, exist_ok=True)

    print(f"å¼€å§‹å¤„ç†ç›®æ ‡è›‹ç™½è´¨: {target_dms_id}")
    print(f"ç»“æœå°†ä¿å­˜è‡³: {final_output_dir}")

    try:
        # ç›´æ¥å°†æ–‡ä»¶è·¯å¾„ä¼ é€’ç»™å‡½æ•°ï¼Œè®©å‡½æ•°å†…éƒ¨å¤„ç† None çš„æƒ…å†µ
        combined_training_data = find_and_combine_data(
            target_dms_id=target_dms_id,
            dms_data_dir=args.dms_data_dir,
            metadata_file_path=args.metadata_file
        )
    except Exception as e:
        print(f"[é”™è¯¯] æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥: {e}")
        return

    # æ°¸ä¹…ä¿å­˜æ¨¡å‹çš„é€»è¾‘
    training_data_path = os.path.join(final_output_dir, "temp_training_data.csv")
    training_df = combined_training_data[['mutated_sequence', 'DMS_score']].rename(
        columns={'mutated_sequence': 'sequence', 'DMS_score': 'label'}
    )
    training_df.to_csv(training_data_path, index=False)

    model_output_path = os.path.join(final_output_dir, "scoring_model.joblib")

    train_model(
        data_path=training_data_path,
        model_output_path=model_output_path,
        base_model=args.base_model,
        repr_layer=args.repr_layer,
        batch_size=args.batch_size
    )

    final_output_file = os.path.join(final_output_dir, "prediction_results_scored.csv")

    score_data(
        model_path=model_output_path,
        base_model=args.base_model,
        repr_layer=args.repr_layer,
        batch_size=args.batch_size,
        data_to_score_path=args.target_data_path,
        output_file_path=final_output_file
    )

    try:
        os.remove(training_data_path)
        print(f"å·²æ¸…ç†ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶: {training_data_path}")
    except OSError as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    print("\næµç¨‹å¤„ç†æˆåŠŸå®Œæˆï¼")
    print(f"æœ€ç»ˆæ‰“åˆ†ç»“æœå·²ä¿å­˜åœ¨: {final_output_file}")

if __name__ == "__main__":
    main()

    python run_pipeline.py \
    --target_data_path "../è›‹ç™½è´¨æ•°æ®/A0A192B1T2_9HIV1_Haddox_2018/prediction_results.csv" \
    --dms_data_dir "../è›‹ç™½è´¨æ•°æ®/" \
    --output_base_dir "../è›‹ç™½è´¨æ•°æ®/" \
    --metadata_file "./DMS_substitutions.csv"

