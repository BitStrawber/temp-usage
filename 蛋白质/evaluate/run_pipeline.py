# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import os
import argparse
import re
import tempfile
from tqdm import tqdm
import joblib

# å¯¼å…¥æ ¸å¿ƒåº“
import torch
from sklearn.linear_model import RidgeCV
from transformers import AutoTokenizer, AutoModelForMaskedLM


# ------------------------------------------------------------------------------
# å‡è®¾æ‚¨æœ‰ä¸€ä¸ª utils.py æ–‡ä»¶åŒ…å«ä»¥ä¸‹å‡½æ•°ã€‚
# å¦‚æœæ²¡æœ‰ï¼Œè¯·å°†è¿™äº›è¾…åŠ©å‡½æ•°æ”¾åœ¨æ­¤è„šæœ¬çš„é¡¶éƒ¨æˆ–ä¸€ä¸ªåä¸º utils.py çš„æ–‡ä»¶ä¸­ã€‚
# ------------------------------------------------------------------------------

def check_device():
    """æ£€æŸ¥å¯ç”¨çš„è®¡ç®—è®¾å¤‡ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰ã€‚"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    return device


def load_esm2_model(model_name, device):
    """åŠ è½½ESM-2æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"""
    print(f"æ­£åœ¨ä» '{model_name}' åŠ è½½ ESM-2 æ¨¡å‹...")
    # ESMæ¨¡å‹åœ¨åŠ è½½æ—¶å¯èƒ½ä¼šæ˜¾ç¤ºå…³äºposition_embeddingsçš„è­¦å‘Šï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼Œå¯ä»¥å¿½ç•¥
    model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print("âœ… ESM-2 æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    # --- ä¿®æ”¹ï¼šåªè¿”å›ä¸¤ä¸ªå€¼ ---
    return model, tokenizer


def get_protein_embeddings_batch(sequences, model, tokenizer, device, repr_layer, batch_size):
    """æ‰¹é‡ç”Ÿæˆè›‹ç™½è´¨åºåˆ—çš„åµŒå…¥å‘é‡ã€‚"""
    embeddings = []

    # æŒ‰ç…§æ‰¹æ¬¡å¤„ç†åºåˆ—ä»¥èŠ‚çœå†…å­˜
    for i in tqdm(range(0, len(sequences), batch_size), desc="ç”ŸæˆåµŒå…¥å‘é‡"):
        batch_seqs = sequences[i:i + batch_size]

        # ä½¿ç”¨tokenizerè¿›è¡Œç¼–ç 
        inputs = tokenizer(batch_seqs, return_tensors="pt", padding=True, truncation=True, max_length=1024).to(device)

        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¯·æ±‚çš„å±‚å­˜åœ¨
        num_layers = len(outputs.hidden_states)
        if not (0 <= repr_layer < num_layers):
            raise ValueError(
                f"é”™è¯¯: è¯·æ±‚çš„å±‚ '{repr_layer}' è¶…å‡ºæ¨¡å‹èŒƒå›´ã€‚ "
                f"æ­¤æ¨¡å‹å…±æœ‰ {num_layers} ä¸ªå±‚ (æœ‰æ•ˆç´¢å¼•ä¸º 0 åˆ° {num_layers - 1})ã€‚"
            )

        hidden_states = outputs.hidden_states[repr_layer]

        # å¯¹æ¯ä¸ªåºåˆ—çš„åµŒå…¥å‘é‡å–å¹³å‡å€¼ï¼ˆå»é™¤paddingçš„å½±å“ï¼‰
        for j, seq in enumerate(batch_seqs):
            seq_len = inputs['attention_mask'][j].sum().item()
            seq_embedding = hidden_states[j, 1:seq_len - 1].mean(dim=0).cpu().numpy()
            embeddings.append(seq_embedding)

    return np.array(embeddings)


# ==============================================================================
# é›†æˆæ¨¡å‹è®­ç»ƒé€»è¾‘ (åµŒå…¥å‘é‡ + å›å½’æ¨¡å‹)
# ==============================================================================

def train_model(data_path: str, model_output_path: str, base_model: str, repr_layer: int, batch_size: int):
    """
    ä½¿ç”¨ESM-2ç”ŸæˆåµŒå…¥å‘é‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå›å½’æ¨¡å‹æ¥é¢„æµ‹åˆ†æ•°ã€‚
    """
    print("=" * 80)
    print(f"å¼€å§‹è®­ç»ƒæ‰“åˆ†æ¨¡å‹ (åµŒå…¥+å›å½’)...")
    print(f" -> è®­ç»ƒæ•°æ®: {data_path}")
    print(f" -> ESM-2æ¨¡å‹: {base_model} (ä½¿ç”¨ç¬¬ {repr_layer} å±‚)")
    print(f" -> æ¨¡å‹å°†ä¿å­˜è‡³: {model_output_path}")

    # 1. åˆå§‹åŒ–è®¾å¤‡å’ŒESM-2æ¨¡å‹
    device = check_device()
    # --- ä¿®æ”¹ï¼šæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼ ---
    esm_model, tokenizer = load_esm2_model(base_model, device)

    # 2. åŠ è½½è®­ç»ƒæ•°æ®
    df_train = pd.read_csv(data_path)
    sequences = df_train['sequence'].tolist()
    labels = df_train['label'].values
    print(f"å…± {len(sequences)} æ¡åºåˆ—ç”¨äºè®­ç»ƒã€‚")

    # 3. ç”Ÿæˆè›‹ç™½è´¨åµŒå…¥å‘é‡
    # --- ä¿®æ”¹ï¼šç§»é™¤ batch_converter å‚æ•° ---
    embeddings = get_protein_embeddings_batch(
        sequences, esm_model, tokenizer, device, repr_layer, batch_size
    )

    # 4. è®­ç»ƒå›å½’æ¨¡å‹
    print("åµŒå…¥å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œå¼€å§‹è®­ç»ƒå›å½’æ¨¡å‹...")
    scoring_model = RidgeCV(alphas=np.logspace(-3, 3, 10), cv=5)
    scoring_model.fit(embeddings, labels)
    print(f"å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆã€‚äº¤å‰éªŒè¯é€‰å‡ºçš„æœ€ä½³alphaå€¼ä¸º: {scoring_model.alpha_:.4f}")

    # 5. ä¿å­˜è®­ç»ƒå¥½çš„å›å½’æ¨¡å‹
    joblib.dump(scoring_model, model_output_path)
    print(f"âœ… æ¨¡å‹å·²æˆåŠŸä¿å­˜è‡³: {model_output_path}")
    print("=" * 80)

# ==============================================================================
# é›†æˆæ•°æ®æ‰“åˆ†é€»è¾‘ (åµŒå…¥å‘é‡ + å›å½’æ¨¡å‹)
# ==============================================================================

def score_data(model_path: str, base_model: str, repr_layer: int, batch_size: int, data_to_score_path: str,
               output_file_path: str):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„å›å½’æ¨¡å‹ï¼Œä¸ºæ–°çš„è›‹ç™½è´¨åºåˆ—è¿›è¡Œæ‰“åˆ†ã€‚
    """
    print("=" * 80)
    print(f"å¼€å§‹ä½¿ç”¨å›å½’æ¨¡å‹ä¸ºå¤šä¸ªåºåˆ—åˆ—è¿›è¡Œæ‰“åˆ†...")
    print(f" -> æ‰“åˆ†æ¨¡å‹: {model_path}")
    print(f" -> ESM-2æ¨¡å‹: {base_model} (ä½¿ç”¨ç¬¬ {repr_layer} å±‚)")
    print(f" -> å¾…æ‰“åˆ†æ•°æ®: {data_to_score_path}")

    # 1. åˆå§‹åŒ–å’ŒåŠ è½½
    device = check_device()
    df = pd.read_csv(data_to_score_path)
    scoring_model = joblib.load(model_path)
    # --- ä¿®æ”¹ï¼šæ¥æ”¶ä¸¤ä¸ªè¿”å›å€¼ (æ­¤è¡Œå·²æ­£ç¡®ï¼Œä½†ä¸ºä¿æŒä¸€è‡´æ€§è€Œå±•ç¤º) ---
    esm_model, tokenizer = load_esm2_model(base_model, device)

    # 2. å®šä¹‰éœ€è¦è¢«æ‰“åˆ†çš„åºåˆ—åˆ—
    sequence_columns_to_score = [
        'mutated_sequence',
        'predicted_sequence_finetuned',
        'predicted_sequence_base'
    ]

    for col in sequence_columns_to_score:
        print(f"\n--- æ­£åœ¨å¤„ç†åˆ—: '{col}' ---")

        if col not in df.columns:
            print(f"âš ï¸ è­¦å‘Š: æŒ‡å®šçš„åˆ— '{col}' åœ¨CSVæ–‡ä»¶ä¸­ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ã€‚")
            continue

        sequences_to_score = df[col].dropna().tolist()
        if not sequences_to_score:
            print(f"â„¹ï¸ åˆ— '{col}' ä¸­æ²¡æœ‰æœ‰æ•ˆåºåˆ—ï¼Œè·³è¿‡ã€‚")
            continue

        # --- ä¿®æ”¹ï¼šç§»é™¤ batch_converter å‚æ•° ---
        embeddings = get_protein_embeddings_batch(
            sequences_to_score, esm_model, tokenizer, device, repr_layer, batch_size
        )

        print("åµŒå…¥å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œå¼€å§‹é¢„æµ‹åˆ†æ•°...")
        predictions = scoring_model.predict(embeddings)

        pred_series = pd.Series(predictions, index=df[col].dropna().index)

        output_col_name = f"predicted_score_{col}"
        df[output_col_name] = pred_series
        df[output_col_name] = df[output_col_name].round(9)
        print(f"âœ… å·²ç”Ÿæˆé¢„æµ‹åˆ†æ•°å¹¶æ·»åŠ åˆ°æ–°åˆ— '{output_col_name}'ã€‚")

    # ... åç»­åˆ†æå’Œä¿å­˜éƒ¨åˆ†æ— éœ€ä¿®æ”¹ ...
    print("\n--- æ­£åœ¨è¿›è¡Œä¼˜åŒ–åˆ†æå’Œæ ‡æ³¨ ---")
    score_finetuned = 'predicted_score_predicted_sequence_finetuned'
    score_base = 'predicted_score_predicted_sequence_base'
    original_score_col = 'DMS_score'
    required_score_cols = [score_finetuned, score_base, original_score_col]

    improvement_count = 0
    if all(col in df.columns for col in required_score_cols):
        condition = (df[score_finetuned] > df[score_base]) & \
                    (df[score_base] > df[original_score_col])
        df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = np.where(condition, 'æ˜¯', 'å¦')
        improvement_count = df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'].value_counts().get('æ˜¯', 0)
        print(f"åˆ†æå®Œæˆã€‚æ–°åˆ— 'æ˜¯å¦ä¼˜åŒ–æˆåŠŸ' å·²æ·»åŠ ã€‚")
        print(f"ğŸ“ˆ ä¼˜åŒ–æˆåŠŸ (å¾®è°ƒåˆ† > åŸºç¡€åˆ† > åŸå§‹DMSåˆ†) çš„åºåˆ—æ•°é‡: {improvement_count}")
    else:
        missing_cols = [col for col in required_score_cols if col not in df.columns]
        print(f"âš ï¸ è­¦å‘Š: ç¼ºå°‘è¿›è¡Œåˆ†ææ‰€éœ€çš„åˆ—: {missing_cols}ã€‚å·²è·³è¿‡ä¼˜åŒ–åˆ†æã€‚")
        df['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = 'åˆ†æè·³è¿‡'

    summary_data = {col: '' for col in df.columns}
    summary_data[df.columns[0]] = "--- ç»Ÿè®¡ç»“æœ ---"
    if 'æ˜¯å¦ä¼˜åŒ–æˆåŠŸ' in df.columns and improvement_count > 0:
        summary_data['æ˜¯å¦ä¼˜åŒ–æˆåŠŸ'] = f"æ€»è®¡ä¼˜åŒ–æˆåŠŸ: {improvement_count}"

    summary_df = pd.DataFrame([summary_data])
    results_df = pd.concat([df, summary_df], ignore_index=True)

    results_df.to_csv(output_file_path, index=False, float_format='%.9f')
    print(f"\nâœ… æ‰“åˆ†å’Œåˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
    print("=" * 80)

# ==============================================================================
# è‡ªåŠ¨åŒ–æµç¨‹çš„æ ¸å¿ƒé€»è¾‘
# ==============================================================================

def extract_dms_id_from_path(path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–DMS_idã€‚"""
    match = re.search(r'\((.*?)\)([^/]+)', path)
    if match:
        return match.group(2).strip()
    else:
        try:
            return os.path.basename(os.path.dirname(path))
        except Exception:
            raise ValueError(f"æ— æ³•ä»è·¯å¾„ {path} ä¸­æå–DMS_idã€‚")


def find_and_combine_data(target_dms_id: str, metadata_df: pd.DataFrame, dms_data_dir: str) -> pd.DataFrame:
    """æ ¹æ®å…ƒæ•°æ®æŸ¥æ‰¾å¹¶åˆå¹¶æ•°æ®é›†ç”¨äºè®­ç»ƒã€‚"""
    target_protein_metadata = metadata_df[metadata_df['DMS_id'] == target_dms_id]
    if target_protein_metadata.empty:
        raise ValueError(f"DMS_id '{target_dms_id}' åœ¨å…ƒæ•°æ®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ã€‚")

    target_assay = target_protein_metadata['selection_assay'].iloc[0]
    target_title = target_protein_metadata['title'].iloc[0]

    print(f"\nç›®æ ‡è›‹ç™½è´¨ '{target_dms_id}' çš„å±æ€§:")
    print(f" -> selection_assay: {target_assay}")
    print(f" -> title: {target_title}")

    matching_proteins = metadata_df[
        (metadata_df['selection_assay'] == target_assay) &
        (metadata_df['title'] == target_title)
        ]

    print(f"\næ‰¾åˆ° {len(matching_proteins)} ä¸ªåŒ¹é…çš„æ•°æ®é›†ç”¨äºæ•°æ®æ‰©å±•:")

    combined_df_list = []
    for _, row in matching_proteins.iterrows():
        dms_file = row['DMS_filename']
        file_path = os.path.join(dms_data_dir, dms_file)
        print(f" -> åŠ è½½æ•°æ®: {dms_file}")

        if not os.path.exists(file_path):
            print(f"    [è­¦å‘Š] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡: {file_path}")
            continue
        try:
            df = pd.read_csv(file_path)
            if 'mutated_sequence' in df.columns and 'DMS_score' in df.columns:
                combined_df_list.append(df[['mutated_sequence', 'DMS_score']])
            else:
                print(f"    [è­¦å‘Š] æ–‡ä»¶ {dms_file} ç¼ºå°‘ 'mutated_sequence' æˆ– 'DMS_score' åˆ—ï¼Œå·²è·³è¿‡ã€‚")
        except Exception as e:
            print(f"    [é”™è¯¯] å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}ã€‚å·²è·³è¿‡ã€‚")

    if not combined_df_list:
        raise RuntimeError("æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®æ¥æ„å»ºæ‰©å±•è®­ç»ƒé›†ã€‚")

    combined_df = pd.concat(combined_df_list, ignore_index=True)
    print(f"\næ•°æ®é›†åˆå¹¶å®Œæˆï¼Œå…± {len(combined_df)} æ¡è®°å½•ç”¨äºè®­ç»ƒã€‚")
    return combined_df


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨æ‰©å±•æ•°æ®è‡ªåŠ¨åŒ–è›‹ç™½è´¨æ‰“åˆ†æµç¨‹ã€‚")
    parser.add_argument("--target_data_path", type=str, required=True, help="å¾…æ‰“åˆ†çš„ç›®æ ‡æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--metadata_file", type=str, required=True, help="å…ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„ ('å·¥ä½œç°¿1.csv')ã€‚")
    parser.add_argument("--dms_data_dir", type=str, required=True, help="åŒ…å«æ‰€æœ‰DMSåŸå§‹æ•°æ®çš„CSVæ–‡ä»¶çš„ç›®å½•ã€‚")
    parser.add_argument("--output_base_dir", type=str, required=True, help="ä¿å­˜æœ€ç»ˆç»“æœçš„åŸºç¡€ç›®å½•ã€‚")

    parser.add_argument("--base_model", type=str, default="../../../esm2_model_local",
                        help="ç”¨äºç”ŸæˆåµŒå…¥å‘é‡çš„ESM-2æ¨¡å‹è·¯å¾„ã€‚")
    parser.add_argument("--repr_layer", type=int, default=12,
                        help="ä»ESM-2æ¨¡å‹çš„å“ªä¸€å±‚æå–è¡¨å¾ã€‚å¯¹äº33å±‚æ¨¡å‹ï¼Œ33æ˜¯æœ€åä¸€å±‚ã€‚")
    parser.add_argument("--batch_size", type=int, default=16, help="ç”ŸæˆåµŒå…¥å‘é‡æ—¶çš„æ‰¹å¤„ç†å¤§å°ã€‚")
    args = parser.parse_args()

    target_dms_id = extract_dms_id_from_path(args.target_data_path)

    original_folder_name = os.path.basename(os.path.dirname(args.target_data_path))
    result_folder_name = f"{original_folder_name}"
    final_output_dir = os.path.join(args.output_base_dir, result_folder_name)
    os.makedirs(final_output_dir, exist_ok=True)

    print(f"å¼€å§‹å¤„ç†ç›®æ ‡è›‹ç™½è´¨: {target_dms_id}")
    print(f"ç»“æœå°†ä¿å­˜è‡³: {final_output_dir}")

    try:
        metadata_df = pd.read_csv(args.metadata_file)
        combined_training_data = find_and_combine_data(target_dms_id, metadata_df, args.dms_data_dir)
    except Exception as e:
        print(f"[é”™è¯¯] æ•°æ®å‡†å¤‡é˜¶æ®µå¤±è´¥: {e}")
        return

    # ä½¿ç”¨ä¸´æ—¶ç›®å½•ç®¡ç†ä¸­é—´æ–‡ä»¶
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_data_path = os.path.join(temp_dir, "training_data.csv")

        training_df = combined_training_data[['mutated_sequence', 'DMS_score']].rename(
            columns={'mutated_sequence': 'sequence', 'DMS_score': 'label'}
        )
        training_df.to_csv(temp_data_path, index=False)

        temp_model_path = os.path.join(temp_dir, "scoring_model.joblib")

        # è®­ç»ƒå›å½’æ¨¡å‹
        train_model(
            data_path=temp_data_path,
            model_output_path=temp_model_path,
            base_model=args.base_model,
            repr_layer=args.repr_layer,
            batch_size=args.batch_size
        )

        final_output_file = os.path.join(final_output_dir, "prediction_results_scored.csv")

        # ä½¿ç”¨å›å½’æ¨¡å‹è¿›è¡Œæ‰“åˆ†
        score_data(
            model_path=temp_model_path,
            base_model=args.base_model,
            repr_layer=args.repr_layer,
            batch_size=args.batch_size,
            data_to_score_path=args.target_data_path,
            output_file_path=final_output_file
        )

    print("\næµç¨‹å¤„ç†æˆåŠŸå®Œæˆï¼")
    print(f"æœ€ç»ˆæ‰“åˆ†ç»“æœå·²ä¿å­˜åœ¨: {final_output_file}")


if __name__ == "__main__":

    main()

    python run_pipeline.py \
    --target_data_path "../../../æ€»ç»“æœ/(yeast growth)DLG4_HUMAN_Faure_2021/prediction_results_from_low_scores.csv" \
    --metadata_file "../../../DMS_substitutions.csv" \
    --dms_data_dir "../../../è›‹ç™½è´¨æ•°æ®/" \
    --output_base_dir "../../../æ€»ç»“æœ/"

autodl-tmp/æ€»ç»“æœ/(yeast growth)DLG4_HUMAN_Faure_2021


